# -*- coding: utf-8 -*-
"""
é€šè¾¾ä¿¡è‚¡ç¥¨æ± åˆ†æžå·¥å…·
ç”¨äºŽè¯»å–é€šè¾¾ä¿¡å¯¼å‡ºçš„è‚¡ç¥¨åˆ—è¡¨å¹¶è¿›è¡Œåˆ†æž
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path


def read_tdx_export(file_path):
    """
    è¯»å–é€šè¾¾ä¿¡å¯¼å‡ºçš„è‚¡ç¥¨æ•°æ®
    æ”¯æŒ CSVã€TXTã€XLS æ ¼å¼
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None, None
    
    suffix = file_path.suffix.lower()
    stock_info = None
    
    try:
        if suffix == '.csv':
            for encoding in ['gbk', 'utf-8', 'gb2312']:
                try:
                    df = pd.read_csv(file_path, encoding=encoding)
                    print(f"æˆåŠŸè¯»å– CSV æ–‡ä»¶ï¼Œç¼–ç : {encoding}")
                    return df, stock_info
                except UnicodeDecodeError:
                    continue
        
        elif suffix == '.txt':
            for encoding in ['gbk', 'utf-8', 'gb2312']:
                try:
                    df = pd.read_csv(file_path, sep='\t', encoding=encoding)
                    print(f"æˆåŠŸè¯»å– TXT æ–‡ä»¶ï¼Œç¼–ç : {encoding}")
                    return df, stock_info
                except UnicodeDecodeError:
                    continue
        
        elif suffix in ['.xls', '.xlsx']:
            import re
            # é€šè¾¾ä¿¡å¯¼å‡ºçš„ xls å®žé™…ä¸Šæ˜¯åˆ¶è¡¨ç¬¦åˆ†éš”çš„æ–‡æœ¬æ–‡ä»¶
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    lines = f.readlines()
                
                # ç¬¬ä¸€è¡Œæ˜¯è‚¡ç¥¨åç§°
                first_line = lines[0].strip()
                match = re.search(r'([\u4e00-\u9fa5]+)\s*[\(ï¼ˆ]([0-9]+)[\)ï¼‰]', first_line)
                if match:
                    stock_info = {'name': match.group(1), 'code': match.group(2)}
                
                # æ‰¾åˆ°è¡¨å¤´è¡Œï¼ˆåŒ…å«"æ—¶é—´"çš„è¡Œï¼‰
                header_idx = 0
                for i, line in enumerate(lines):
                    if 'æ—¶é—´' in line:
                        header_idx = i
                        break
                
                # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”è¯»å–
                df = pd.read_csv(file_path, sep='\t', encoding='gbk', 
                                skiprows=header_idx, skipinitialspace=True)
                
                # æ¸…ç†åˆ—åä¸­çš„ç©ºç™½
                df.columns = [str(c).strip() for c in df.columns]
                
                print(f"æˆåŠŸè¯»å– XLS æ–‡ä»¶ (åˆ¶è¡¨ç¬¦åˆ†éš”)")
                return df, stock_info
            except Exception as e:
                print(f"åˆ¶è¡¨ç¬¦æ ¼å¼è¯»å–å¤±è´¥: {e}")
            
            # å°è¯•æ ‡å‡†Excelæ ¼å¼
            try:
                df = pd.read_excel(file_path)
                print("æˆåŠŸè¯»å– Excel æ–‡ä»¶")
                return df, stock_info
            except:
                pass
            
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
        return None, None
    
    print("æ— æ³•è§£æžæ–‡ä»¶")
    return None, None


def clean_column_names(df):
    """
    æ¸…ç†å’Œæ ‡å‡†åŒ–åˆ—å
    """
    col_mapping = {
        'æ—¶é—´': 'date',
        'å¼€ç›˜': 'open',
        'æœ€é«˜': 'high',
        'æœ€ä½Ž': 'low',
        'æ”¶ç›˜': 'close',
        'æˆäº¤é‡': 'volume',
    }
    
    new_cols = []
    for col in df.columns:
        col_str = str(col).strip()
        if col_str in col_mapping:
            new_cols.append(col_mapping[col_str])
        else:
            new_cols.append(col_str)
    df.columns = new_cols
    return df


def analyze_kline(df, stock_info=None):
    """
    åˆ†æžKçº¿æ•°æ®
    """
    if df is None or df.empty:
        print("æ²¡æœ‰æ•°æ®å¯åˆ†æž")
        return None
    
    # æ¸…ç†åˆ—å
    df = clean_column_names(df)
    
    # åˆ é™¤å…¨ç©ºè¡Œ
    df = df.dropna(how='all')
    
    print("\n" + "=" * 60)
    if stock_info:
        print(f"ðŸ“Š {stock_info['name']} ({stock_info['code']}) Kçº¿åˆ†æž")
    else:
        print("ðŸ“Š Kçº¿æ•°æ®åˆ†æž")
    print("=" * 60)
    
    print(f"\nðŸ“Œ æ•°æ®æ¡æ•°: {len(df)} æ¡")
    
    # è¯†åˆ«åŸºæœ¬åˆ—
    if 'date' in df.columns:
        print(f"ðŸ“… æ—¶é—´èŒƒå›´: {df['date'].iloc[0]} ~ {df['date'].iloc[-1]}")
    
    # ä»·æ ¼åˆ†æž
    if 'close' in df.columns:
        close = pd.to_numeric(df['close'], errors='coerce')
        print(f"\nðŸ’° ä»·æ ¼ç»Ÿè®¡:")
        print(f"   æœ€æ–°æ”¶ç›˜: {close.iloc[-1]:.2f}")
        print(f"   æœ€é«˜ä»·æ ¼: {close.max():.2f}")
        print(f"   æœ€ä½Žä»·æ ¼: {close.min():.2f}")
        print(f"   å¹³å‡ä»·æ ¼: {close.mean():.2f}")
        
        # è®¡ç®—æ¶¨è·Œå¹…
        if len(close) > 1:
            total_change = (close.iloc[-1] - close.iloc[0]) / close.iloc[0] * 100
            print(f"   åŒºé—´æ¶¨è·Œ: {total_change:+.2f}%")
    
    # æˆäº¤é‡åˆ†æž
    if 'volume' in df.columns:
        vol = pd.to_numeric(df['volume'], errors='coerce')
        print(f"\nðŸ“Š æˆäº¤é‡ç»Ÿè®¡:")
        print(f"   å¹³å‡æˆäº¤: {vol.mean()/10000:.2f} ä¸‡")
        print(f"   æœ€å¤§æˆäº¤: {vol.max()/10000:.2f} ä¸‡")
    
    # MACDåˆ†æž
    macd_cols = [c for c in df.columns if 'MACD' in str(c).upper()]
    if macd_cols:
        print(f"\nðŸ“ˆ MACDæŒ‡æ ‡ (æœ€æ–°):")
        for col in macd_cols:
            val = pd.to_numeric(df[col], errors='coerce').iloc[-1]
            if not pd.isna(val):
                print(f"   {col}: {val:.4f}")
    
    print(f"\nðŸ“‹ æ‰€æœ‰åˆ—: {list(df.columns)}")
    
    return df


def export_filtered(df, output_path, condition=None):
    """
    æ ¹æ®æ¡ä»¶ç­›é€‰å¹¶å¯¼å‡º
    """
    if condition:
        filtered_df = df.query(condition)
    else:
        filtered_df = df
    
    filtered_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"å·²å¯¼å‡º {len(filtered_df)} æ¡æ•°æ®åˆ°: {output_path}")
    return filtered_df


def read_stock_list(file_path):
    """
    è¯»å–é€šè¾¾ä¿¡å¯¼å‡ºçš„è‡ªé€‰è‚¡åˆ—è¡¨
    """
    file_path = Path(file_path)
    
    try:
        with open(file_path, 'r', encoding='gbk') as f:
            lines = f.readlines()
        
        # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”è¯»å–
        df = pd.read_csv(file_path, sep='\t', encoding='gbk', skipinitialspace=True)
        
        # æ¸…ç†åˆ—å
        df.columns = [str(c).strip() for c in df.columns]
        
        # æ¸…ç†ä»£ç åˆ—ï¼ˆåŽ»æŽ‰="ï¼‰
        if 'ä»£ç ' in df.columns:
            df['ä»£ç '] = df['ä»£ç '].astype(str).str.replace('="', '').str.replace('"', '')
        
        # è¿‡æ»¤æŽ‰æ³¨é‡Šè¡Œ
        df = df[~df.iloc[:, 0].astype(str).str.startswith('#')]
        
        print(f"æˆåŠŸè¯»å–è‡ªé€‰è‚¡åˆ—è¡¨")
        return df
    except Exception as e:
        print(f"è¯»å–å¤±è´¥: {e}")
        return None


def analyze_stock_list(df):
    """
    åˆ†æžè‡ªé€‰è‚¡åˆ—è¡¨ï¼ˆæ”¯æŒåˆ†ç»„æ ‡è®°ï¼‰
    """
    if df is None or df.empty:
        print("æ²¡æœ‰æ•°æ®")
        return
    
    # è¯†åˆ«åˆ†ç»„æ ‡è®°è¡Œå’Œè‚¡ç¥¨è¡Œ
    groups = {}
    current_group = "æœªåˆ†ç±»"
    
    for idx, row in df.iterrows():
        code = str(row.get('ä»£ç ', '')).strip()
        name = str(row.get('åç§°', '')).strip()
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºåˆ†ç»„æ ‡è®°è¡Œï¼ˆä»£ç åˆ—æ˜¯ä¸­æ–‡æˆ–ç‰¹æ®Šæ ‡è®°ï¼‰
        if not code or not code[0].isdigit():
            # è¿™æ˜¯åˆ†ç»„æ ‡è®°è¡Œ
            group_name = code if code else name
            if group_name and 'é‡å¤' not in group_name and 'æ•°æ®æ¥æº' not in group_name:
                current_group = group_name
                if current_group not in groups:
                    groups[current_group] = []
        else:
            # è¿™æ˜¯è‚¡ç¥¨è¡Œ
            if current_group not in groups:
                groups[current_group] = []
            groups[current_group].append(row)
    
    # è½¬æ¢ä¸ºDataFrame
    group_dfs = {}
    for g, rows in groups.items():
        if rows:
            group_dfs[g] = pd.DataFrame(rows)
    
    # åˆ†ç¦»å¤§ç›˜æŒ‡æ•°
    all_stocks = pd.concat(group_dfs.values()) if group_dfs else df
    index_df = all_stocks[all_stocks['ä»£ç '].astype(str).str.startswith('99')]
    
    # ç»Ÿè®¡é‡å¤è‚¡ç¥¨
    all_codes = []
    for g, gdf in group_dfs.items():
        codes = gdf[~gdf['ä»£ç '].astype(str).str.startswith('99')]['ä»£ç '].tolist()
        all_codes.extend([(c, g) for c in codes])
    
    from collections import Counter
    code_counts = Counter([c for c, g in all_codes])
    duplicates = {c: [] for c, cnt in code_counts.items() if cnt > 1}
    for c, g in all_codes:
        if c in duplicates:
            duplicates[c].append(g)
    
    # åˆå¹¶æ‰€æœ‰ä¸ªè‚¡å¹¶åŽ»é‡
    stock_df = all_stocks[~all_stocks['ä»£ç '].astype(str).str.startswith('99')]
    stock_df = stock_df.drop_duplicates(subset=['ä»£ç '])
    
    print("\n" + "=" * 60)
    print("ðŸ“Š è‡ªé€‰è‚¡æ± åˆ†æž")
    print("=" * 60)
    
    # ===== å¤§ç›˜æŒ‡æ•°å•ç‹¬åˆ†æž =====
    if not index_df.empty:
        print("\n" + "â”€" * 40)
        print("ðŸ“ˆ ã€å¤§ç›˜æŒ‡æ•°ã€‘")
        print("â”€" * 40)
        for _, row in index_df.iterrows():
            name = row.get('åç§°', '')
            price = row.get('çŽ°ä»·', 0)
            change = row.get('æ¶¨å¹…%', 0)
            try:
                print(f"   {name}: {price}  ({float(change):+.2f}%)")
            except:
                print(f"   {name}: {price}")
    
    # ===== é‡å¤è‚¡ç¥¨ï¼ˆå¤šé‡ä¿¡å·ï¼‰=====
    if duplicates:
        print("\n" + "â”€" * 40)
        print("â­ ã€å¤šé‡ä¿¡å·è‚¡ç¥¨ã€‘è¢«å¤šä¸ªå…¬å¼åŒæ—¶é€‰ä¸­")
        print("â”€" * 40)
        for code, grps in duplicates.items():
            stock_row = stock_df[stock_df['ä»£ç '] == code]
            if not stock_row.empty:
                name = stock_row.iloc[0].get('åç§°', '')
                print(f"   ðŸ”¥ {name} ({code})")
                print(f"      å‡ºçŽ°åœ¨: {' + '.join(grps)}")
    
    # ===== æŒ‰å…¬å¼åˆ†ç»„æ˜¾ç¤º =====
    print("\n" + "â”€" * 40)
    print("ðŸ“‹ ã€æŒ‰å…¬å¼åˆ†ç»„ã€‘")
    print("â”€" * 40)
    
    for group_name, gdf in group_dfs.items():
        # è¿‡æ»¤æŽ‰æŒ‡æ•°
        gdf_stocks = gdf[~gdf['ä»£ç '].astype(str).str.startswith('99')]
        if gdf_stocks.empty:
            continue
            
        print(f"\nâ–¶ {group_name} ({len(gdf_stocks)}åª)")
        
        # æŒ‰æ¶¨å¹…æŽ’åº
        if 'æ¶¨å¹…%' in gdf_stocks.columns:
            gdf_stocks = gdf_stocks.copy()
            gdf_stocks['æ¶¨å¹…%'] = pd.to_numeric(gdf_stocks['æ¶¨å¹…%'], errors='coerce')
            gdf_stocks = gdf_stocks.sort_values('æ¶¨å¹…%', ascending=False)
        
        for _, row in gdf_stocks.iterrows():
            code = str(row.get('ä»£ç ', ''))
            name = str(row.get('åç§°', ''))
            change = row.get('æ¶¨å¹…%', 0)
            dup_mark = " â­" if code in duplicates else ""
            try:
                print(f"   {name}: {float(change):+.2f}%{dup_mark}")
            except:
                print(f"   {name}{dup_mark}")
    
    # ===== æ±‡æ€»ç»Ÿè®¡ =====
    print("\n" + "â”€" * 40)
    print(f"ðŸ“Š ã€æ±‡æ€»ç»Ÿè®¡ã€‘å…± {len(stock_df)} åªï¼ˆå·²åŽ»é‡ï¼‰")
    print("â”€" * 40)
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "â”€" * 40)
    print("ðŸ“Š ã€ç»Ÿè®¡ä¿¡æ¯ã€‘")
    print("â”€" * 40)
    
    if 'æ¶¨å¹…%' in stock_df.columns:
        changes = pd.to_numeric(stock_df['æ¶¨å¹…%'], errors='coerce')
        up_count = (changes > 0).sum()
        down_count = (changes < 0).sum()
        flat_count = (changes == 0).sum()
        print(f"   ä¸Šæ¶¨: {up_count} åª  |  ä¸‹è·Œ: {down_count} åª  |  å¹³ç›˜: {flat_count} åª")
        print(f"   æœ€å¤§æ¶¨å¹…: {changes.max():+.2f}%  |  æœ€å¤§è·Œå¹…: {changes.min():+.2f}%")
        print(f"   å¹³å‡æ¶¨å¹…: {changes.mean():+.2f}%")
    
    # æ¶¨å¹…æ¦œ
    print("\nðŸ”¥ æ¶¨å¹…å‰5:")
    for _, row in stock_df.head(5).iterrows():
        print(f"   {row.get('åç§°', '')}: {row.get('æ¶¨å¹…%', 0):+.2f}%")
    
    print("\nâ„ï¸ è·Œå¹…å‰5:")
    for _, row in stock_df.tail(5).iloc[::-1].iterrows():
        print(f"   {row.get('åç§°', '')}: {row.get('æ¶¨å¹…%', 0):+.2f}%")
    
    return stock_df, index_df


def is_stock_list(file_path):
    """åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªé€‰è‚¡åˆ—è¡¨æ ¼å¼"""
    try:
        with open(file_path, 'r', encoding='gbk') as f:
            first_line = f.readline()
            return 'ä»£ç ' in first_line and 'åç§°' in first_line
    except:
        return False


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

if __name__ == "__main__":
    print("=" * 60)
    print("é€šè¾¾ä¿¡è‚¡ç¥¨æ•°æ®åˆ†æžå·¥å…·")
    print("=" * 60)
    
    export_dir = Path(__file__).parent
    
    # æŸ¥æ‰¾ç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶ï¼ˆæŽ’é™¤é…ç½®æ–‡ä»¶ï¼‰
    data_files = [f for f in export_dir.glob("*.csv") if 'requirements' not in f.name.lower()] + \
                 list(export_dir.glob("*.xls*"))
    
    if not data_files:
        print("\nâš ï¸ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼")
        print("\nè¯·ä»Žé€šè¾¾ä¿¡å¯¼å‡ºæ•°æ®åŽæ”¾åˆ°æ­¤ç›®å½•ã€‚")
    else:
        print(f"\næ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
        for i, f in enumerate(data_files, 1):
            print(f"  {i}. {f.name}")
        
        # åˆ†æžæ‰€æœ‰æ–‡ä»¶
        for file_path in data_files:
            print(f"\n{'â”' * 60}")
            print(f"ðŸ“ æ–‡ä»¶: {file_path.name}")
            print(f"{'â”' * 60}")
            
            # åˆ¤æ–­æ–‡ä»¶ç±»åž‹
            if is_stock_list(file_path):
                # è‡ªé€‰è‚¡åˆ—è¡¨æ ¼å¼
                df = read_stock_list(file_path)
                if df is not None:
                    analyze_stock_list(df)
            else:
                # Kçº¿æ•°æ®æ ¼å¼
                df, stock_info = read_tdx_export(file_path)
                if df is not None:
                    analyze_kline(df, stock_info)
